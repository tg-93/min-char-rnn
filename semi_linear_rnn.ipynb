{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tg-93/min-char-rnn/blob/main/semi_linear_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vowXRstocd8F",
        "outputId": "1c953dc2-fb08-4d09-e0a7-2b16ab86c123",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1630939 characters, 76 unique.\n",
            "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '$', 5: \"'\", 6: '(', 7: ')', 8: ',', 9: '-', 10: '.', 11: '0', 12: '1', 13: '2', 14: '3', 15: '4', 16: '5', 17: '6', 18: '7', 19: '8', 20: '9', 21: ':', 22: ';', 23: '?', 24: 'A', 25: 'B', 26: 'C', 27: 'D', 28: 'E', 29: 'F', 30: 'G', 31: 'H', 32: 'I', 33: 'J', 34: 'K', 35: 'L', 36: 'M', 37: 'N', 38: 'O', 39: 'P', 40: 'Q', 41: 'R', 42: 'S', 43: 'T', 44: 'U', 45: 'V', 46: 'W', 47: 'Y', 48: 'Z', 49: '`', 50: 'a', 51: 'b', 52: 'c', 53: 'd', 54: 'e', 55: 'f', 56: 'g', 57: 'h', 58: 'i', 59: 'j', 60: 'k', 61: 'l', 62: 'm', 63: 'n', 64: 'o', 65: 'p', 66: 'q', 67: 'r', 68: 's', 69: 't', 70: 'u', 71: 'v', 72: 'w', 73: 'x', 74: 'y', 75: 'z'}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "\n",
        "data = open('wot1.txt', 'r').read() # should be simple plain text file\n",
        "chars = sorted(list(set(data)))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "print(ix_to_char)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "  def __init__(self, fan_in, fan_out, device='cpu'):\n",
        "    self.w = torch.randn(fan_in, fan_out, device=device) / fan_in**0.5\n",
        "    self.b = torch.zeros(1, fan_out, requires_grad=True, device = device)\n",
        "    self.w.requires_grad = True\n",
        "\n",
        "  def __call__(self, input):\n",
        "    return input @ self.w + self.b\n",
        "\n",
        "  def sample(self, input):\n",
        "    return self.__call__(input)\n",
        "\n",
        "  def params(self):\n",
        "    return [self.w, self.b]\n",
        "\n",
        "  def is_stateful(self):\n",
        "    return False\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"Linear: {}\".format(self.w.shape)\n",
        "\n",
        "class Tanh:\n",
        "  def __call__(self, input):\n",
        "    self.out = torch.tanh(input)\n",
        "    return self.out\n",
        "\n",
        "  def sample(self, input):\n",
        "    return self.__call__(input)\n",
        "\n",
        "  def params(self):\n",
        "    return []\n",
        "\n",
        "  def is_stateful(self):\n",
        "    return False\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"Tanh\""
      ],
      "metadata": {
        "id": "iwENKZS7dT_5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRNN:\n",
        "  def __init__(self, fan_in, hidden_size, batch_size, device='cpu', linear_time=False):\n",
        "    self.hidden_size = hidden_size\n",
        "    self.linear_time = linear_time\n",
        "    self.hidden = torch.zeros(batch_size, hidden_size, requires_grad=True, device=device)\n",
        "    self.lin = Linear(fan_in + hidden_size, hidden_size, device=device)\n",
        "    self.tanh = Tanh()\n",
        "\n",
        "  def is_stateful(self):\n",
        "    return True\n",
        "\n",
        "  def __call__(self, input, sample=False):\n",
        "    xh = torch.hstack((input, self.hidden))\n",
        "    if self.linear_time:\n",
        "      self.hidden = self.lin(xh) # no tanh on feedforward through time\n",
        "      return self.tanh(self.hidden)\n",
        "    self.hidden = self.tanh(self.lin(xh))\n",
        "    return self.hidden\n",
        "\n",
        "  def sample(self, input):\n",
        "    with torch.no_grad():\n",
        "      if self.hidden_sample is None:\n",
        "        self.hidden_sample = self.hidden[0,:].view(1, self.hidden_size)\n",
        "      xh = torch.hstack((input, self.hidden_sample))\n",
        "      if self.linear_time:\n",
        "        self.hidden_sample = self.lin(xh)\n",
        "        return self.tanh(self.hidden_sample)\n",
        "      self.hidden_sample = self.tanh(self.lin(xh))\n",
        "      return self.hidden_sample\n",
        "\n",
        "  def reset(self):\n",
        "    self.hidden = torch.zeros_like(self.hidden)\n",
        "    self.hidden_sample = None\n",
        "\n",
        "  def reset_grads(self):\n",
        "    self.hidden.detach_()\n",
        "\n",
        "  def params(self):\n",
        "    return self.lin.params()\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"VanillaRNN: {}\".format([p.shape for p in self.params()]) if not self.linear_time else \"TimeLinearRNN: {}\".format([p.shape for p in self.params()])\n",
        "\n",
        "class LinearRNN:\n",
        "  def __init__(self, fan_in, hidden_size, batch_size, device='cpu'):\n",
        "    self.hidden_size = hidden_size\n",
        "    self.hidden = torch.zeros(batch_size, hidden_size, requires_grad=True, device=device)\n",
        "    self.lin = Linear(fan_in + hidden_size, hidden_size, device=device)\n",
        "    self.tanh = Tanh() # this is stored to access the outputs stored inside it for analysis\n",
        "\n",
        "  def __call__(self, input, sample=False):\n",
        "    xh = torch.hstack((input, self.hidden))\n",
        "    self.hidden = self.lin(xh) # no tanh on feedforward through time\n",
        "    return self.tanh(self.hidden)\n",
        "\n",
        "  def sample(self, input):\n",
        "    with torch.no_grad():\n",
        "      if self.hidden_sample is None:\n",
        "        self.hidden_sample = self.hidden[0,:].view(1, self.hidden_size)\n",
        "      xh = torch.hstack((input, self.hidden_sample))\n",
        "      self.hidden_sample = self.lin(xh)\n",
        "      return self.tanh(self.hidden_sample)\n",
        "\n",
        "  def reset(self):\n",
        "    self.hidden = torch.zeros_like(self.hidden)\n",
        "    self.hidden_sample = None\n",
        "\n",
        "  def reset_grads(self):\n",
        "    self.hidden.detach_()\n",
        "\n",
        "  def params(self):\n",
        "    return self.lin.params()\n",
        "\n",
        "  def is_stateful(self):\n",
        "    return True\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"LinearRNN: {}\".format([p.shape for p in self.params()])\n",
        "\n",
        "class Sequential:\n",
        "  def __init__(self, vocab_size, layers, device='cpu'):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.layers = layers\n",
        "    self.device = device\n",
        "    params = []\n",
        "    for layer in layers:\n",
        "      params += layer.params()\n",
        "    self.params = params\n",
        "    count = sum([p.nelement() for p in self.params])\n",
        "    print(self)\n",
        "    print(f'Parameter Count: {count}')\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    logits = torch.zeros(inputs.shape[0], inputs.shape[1], self.vocab_size, dtype=torch.float32, device=self.device)\n",
        "    for t in range(inputs.shape[1]):\n",
        "      x = F.one_hot(inputs[:,t], self.vocab_size).float()\n",
        "      for layer in self.layers:\n",
        "        x = layer(x)\n",
        "      logits[:, t] = x\n",
        "    self.reset()\n",
        "    return logits\n",
        "\n",
        "  def sample(self, input, n):\n",
        "    self.reset()\n",
        "    samples = [input]\n",
        "    for t in range(n):\n",
        "      x = F.one_hot(samples[-1], self.vocab_size).float().view(1, self.vocab_size)\n",
        "      for layer in self.layers:\n",
        "        x = layer.sample(x)\n",
        "      probs = torch.softmax(x, dim=1)\n",
        "      samples.append(torch.multinomial(probs, 1)[0])\n",
        "    return samples\n",
        "\n",
        "  def reset(self):\n",
        "    for layer in self.layers:\n",
        "      if layer.is_stateful():\n",
        "        layer.reset()\n",
        "\n",
        "  def __str__(self):\n",
        "    s = \"Sequential:\\n\"\n",
        "    for layer in self.layers:\n",
        "      s += str(layer) + \"\\n\"\n",
        "    return s"
      ],
      "metadata": {
        "id": "uLH_wjTSepPE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "model = Sequential(vocab_size,\n",
        " [VanillaRNN(vocab_size, 500, batch_size, device='cuda', linear_time=True),\n",
        "  VanillaRNN(500, 400, batch_size, device='cuda'),\n",
        "  Linear(400, vocab_size, device='cuda')], device='cuda')"
      ],
      "metadata": {
        "id": "0uLhFjaLesWm",
        "outputId": "b5ee5d01-40c4-478c-8b30-ba9004d8491a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential:\n",
            "VanillaRNN: [torch.Size([576, 500]), torch.Size([1, 500])]\n",
            "VanillaRNN: [torch.Size([900, 400]), torch.Size([1, 400])]\n",
            "Linear: torch.Size([400, 76])\n",
            "\n",
            "Parameter Count: 679376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smooth_loss = -torch.log(torch.tensor(1.0/vocab_size, device='cuda')) # loss at iteration 0\n",
        "batch_gap = int(len(data)//batch_size)\n",
        "seq_length = 64\n",
        "n = 0\n",
        "optimizer = torch.optim.Adam(model.params)\n",
        "input_end = len(data) - seq_length\n",
        "for n in range(50000):\n",
        "  batch_starts = np.random.randint(0, input_end, batch_size)\n",
        "  inputs = torch.tensor([[char_to_ix[ch] for ch in data[b_i : b_i + seq_length]] for b_i in batch_starts], device='cuda')\n",
        "  targets = torch.tensor([[char_to_ix[ch] for ch in data[b_i + 1 : b_i + seq_length + 1]] for b_i in batch_starts], device='cuda')\n",
        "  # sample from the model now and then\n",
        "  if n>0 and n%200 == 0:\n",
        "    samples = model.sample(inputs[0, 0].detach(), 200)\n",
        "    # print(samples)\n",
        "    txt = ''.join(ix_to_char[ix.item()] for ix in samples)\n",
        "    print('*** Sample: ***')\n",
        "    print(f'----\\n{txt}\\n----')\n",
        "  # forward seq_length characters through the net and fetch gradient\n",
        "  logits = model(inputs)\n",
        "  loss = torch.tensor(0.0, device='cuda')\n",
        "  for b_i in range(batch_size):\n",
        "    loss += F.cross_entropy(logits[b_i], targets[b_i])\n",
        "  loss /= batch_size\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  smooth_loss = smooth_loss * 0.995 + loss * 0.005\n",
        "  if n>0 and n % 50 == 0:\n",
        "    print(f'iter: {n}, loss: {smooth_loss.item()}') # print progress\n",
        "    # plt.figure(figsize=(20, 4)) # width and height of the plot\n",
        "    # legends = []\n",
        "    for i, layer in enumerate(model.layers[:-1]): # note: exclude the output layer\n",
        "      if isinstance(layer, LinearRNN) or isinstance(layer, VanillaRNN):\n",
        "        t = layer.tanh.out\n",
        "        print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
        "    #     hy, hx = torch.histogram(t, density=True)\n",
        "    #     plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    #     legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
        "    # plt.legend(legends);\n",
        "    # plt.title('activation distribution')"
      ],
      "metadata": {
        "id": "ODMbbiJ1e_oW",
        "outputId": "35ddb3f4-cf52-4397-d9fe-25a2839d2018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 50, loss: 4.102715492248535\n",
            "layer 0 (VanillaRNN): mean +0.01, std 1.00, saturated: 100.00%\n",
            "layer 1 (VanillaRNN): mean +0.06, std 0.95, saturated: 77.07%\n",
            "iter: 100, loss: 3.8804380893707275\n",
            "layer 0 (VanillaRNN): mean +0.01, std 1.00, saturated: 100.00%\n",
            "layer 1 (VanillaRNN): mean +0.06, std 0.96, saturated: 79.84%\n",
            "iter: 150, loss: 3.7070417404174805\n",
            "layer 0 (VanillaRNN): mean +0.02, std 1.00, saturated: 100.00%\n",
            "layer 1 (VanillaRNN): mean +0.06, std 0.97, saturated: 81.11%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ad445953b985>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# sample from the model now and then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# print(samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2b5dd52fd186>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input, n)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m       \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m       \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
          ]
        }
      ]
    }
  ]
}