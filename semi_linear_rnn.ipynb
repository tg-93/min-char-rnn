{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tg-93/min-char-rnn/blob/main/semi_linear_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vowXRstocd8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de109fe9-7ad6-4144-81e5-aea108b04366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1630939 characters, 76 unique.\n",
            "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '$', 5: \"'\", 6: '(', 7: ')', 8: ',', 9: '-', 10: '.', 11: '0', 12: '1', 13: '2', 14: '3', 15: '4', 16: '5', 17: '6', 18: '7', 19: '8', 20: '9', 21: ':', 22: ';', 23: '?', 24: 'A', 25: 'B', 26: 'C', 27: 'D', 28: 'E', 29: 'F', 30: 'G', 31: 'H', 32: 'I', 33: 'J', 34: 'K', 35: 'L', 36: 'M', 37: 'N', 38: 'O', 39: 'P', 40: 'Q', 41: 'R', 42: 'S', 43: 'T', 44: 'U', 45: 'V', 46: 'W', 47: 'Y', 48: 'Z', 49: '`', 50: 'a', 51: 'b', 52: 'c', 53: 'd', 54: 'e', 55: 'f', 56: 'g', 57: 'h', 58: 'i', 59: 'j', 60: 'k', 61: 'l', 62: 'm', 63: 'n', 64: 'o', 65: 'p', 66: 'q', 67: 'r', 68: 's', 69: 't', 70: 'u', 71: 'v', 72: 'w', 73: 'x', 74: 'y', 75: 'z'}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "\n",
        "data = open('wot1.txt', 'r').read() # should be simple plain text file\n",
        "chars = sorted(list(set(data)))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "print(ix_to_char)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "  def __init__(self, fan_in, fan_out, device='cpu'):\n",
        "    self.w = torch.randn(fan_in, fan_out, device=device) / fan_in**0.5\n",
        "    self.b = torch.zeros(1, fan_out, requires_grad=True, device = device)\n",
        "    self.w.requires_grad = True\n",
        "\n",
        "  def __call__(self, input):\n",
        "    return input @ self.w + self.b\n",
        "\n",
        "  def sample(self, input):\n",
        "    return self.__call__(input)\n",
        "\n",
        "  def params(self):\n",
        "    return [self.w, self.b]\n",
        "\n",
        "  def is_stateful(self):\n",
        "    return False\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"Linear: {}\".format(self.w.shape)\n",
        "\n",
        "class Tanh:\n",
        "  def __call__(self, input):\n",
        "    self.out = torch.tanh(input)\n",
        "    return self.out\n",
        "\n",
        "  def sample(self, input):\n",
        "    return self.__call__(input)\n",
        "\n",
        "  def params(self):\n",
        "    return []\n",
        "\n",
        "  def is_stateful(self):\n",
        "    return False\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"Tanh\""
      ],
      "metadata": {
        "id": "iwENKZS7dT_5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRNN:\n",
        "  def __init__(self, fan_in, hidden_size, batch_size, device='cpu', linear_time=False):\n",
        "    self.hidden_size = hidden_size\n",
        "    self.linear_time = linear_time\n",
        "    self.hidden = torch.zeros(batch_size, hidden_size, requires_grad=True, device=device)\n",
        "    self.lin = Linear(fan_in + hidden_size, hidden_size, device=device)\n",
        "    self.tanh = Tanh()\n",
        "\n",
        "  def is_stateful(self):\n",
        "    return True\n",
        "\n",
        "  def __call__(self, input, sample=False):\n",
        "    xh = torch.hstack((input, self.hidden))\n",
        "    if self.linear_time:\n",
        "      self.hidden = self.lin(xh) # no tanh on feedforward through time\n",
        "      return self.tanh(self.hidden)\n",
        "    self.hidden = self.tanh(self.lin(xh))\n",
        "    return self.hidden\n",
        "\n",
        "  def sample(self, input):\n",
        "    with torch.no_grad():\n",
        "      if self.hidden_sample is None:\n",
        "        self.hidden_sample = self.hidden[0,:].view(1, self.hidden_size)\n",
        "      xh = torch.hstack((input, self.hidden_sample))\n",
        "      if self.linear_time:\n",
        "        self.hidden_sample = self.lin(xh)\n",
        "        return self.tanh(self.hidden_sample)\n",
        "      self.hidden_sample = self.tanh(self.lin(xh))\n",
        "      return self.hidden_sample\n",
        "\n",
        "  def reset(self):\n",
        "    self.hidden = torch.zeros_like(self.hidden)\n",
        "    self.hidden_sample = None\n",
        "\n",
        "  def reset_grads(self):\n",
        "    self.hidden.detach_()\n",
        "\n",
        "  def params(self):\n",
        "    return self.lin.params()\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"VanillaRNN: {}\".format([p.shape for p in self.params()]) if not self.linear_time else \"TimeLinearRNN: {}\".format([p.shape for p in self.params()])\n",
        "\n",
        "class LinearRNN:\n",
        "  def __init__(self, fan_in, hidden_size, batch_size, device='cpu'):\n",
        "    self.hidden_size = hidden_size\n",
        "    self.hidden = torch.zeros(batch_size, hidden_size, requires_grad=True, device=device)\n",
        "    self.lin = Linear(fan_in + hidden_size, hidden_size, device=device)\n",
        "    self.tanh = Tanh() # this is stored to access the outputs stored inside it for analysis\n",
        "\n",
        "  def __call__(self, input, sample=False):\n",
        "    xh = torch.hstack((input, self.hidden))\n",
        "    self.hidden = self.lin(xh) # no tanh on feedforward through time\n",
        "    return self.tanh(self.hidden)\n",
        "\n",
        "  def sample(self, input):\n",
        "    with torch.no_grad():\n",
        "      if self.hidden_sample is None:\n",
        "        self.hidden_sample = self.hidden[0,:].view(1, self.hidden_size)\n",
        "      xh = torch.hstack((input, self.hidden_sample))\n",
        "      self.hidden_sample = self.lin(xh)\n",
        "      return self.tanh(self.hidden_sample)\n",
        "\n",
        "  def reset(self):\n",
        "    self.hidden = torch.zeros_like(self.hidden)\n",
        "    self.hidden_sample = None\n",
        "\n",
        "  def reset_grads(self):\n",
        "    self.hidden.detach_()\n",
        "\n",
        "  def params(self):\n",
        "    return self.lin.params()\n",
        "\n",
        "  def is_stateful(self):\n",
        "    return True\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"LinearRNN: {}\".format([p.shape for p in self.params()])\n",
        "\n",
        "class Sequential:\n",
        "  def __init__(self, vocab_size, layers, device='cpu'):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.layers = layers\n",
        "    self.device = device\n",
        "    params = []\n",
        "    for layer in layers:\n",
        "      params += layer.params()\n",
        "    self.params = params\n",
        "    count = sum([p.nelement() for p in self.params])\n",
        "    print(self)\n",
        "    print(f'Parameter Count: {count}')\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    logits = torch.zeros(inputs.shape[0], inputs.shape[1], self.vocab_size, dtype=torch.float32, device=self.device)\n",
        "    for t in range(inputs.shape[1]):\n",
        "      x = F.one_hot(inputs[:,t], self.vocab_size).float()\n",
        "      for layer in self.layers:\n",
        "        x = layer(x)\n",
        "      logits[:, t] = x\n",
        "    self.reset()\n",
        "    return logits\n",
        "\n",
        "  def sample(self, input, n):\n",
        "    self.reset()\n",
        "    samples = [input]\n",
        "    for t in range(n):\n",
        "      x = F.one_hot(samples[-1], self.vocab_size).float().view(1, self.vocab_size)\n",
        "      for layer in self.layers:\n",
        "        x = layer.sample(x)\n",
        "      probs = torch.softmax(x, dim=1)\n",
        "      samples.append(torch.multinomial(probs, 1)[0])\n",
        "    return samples\n",
        "\n",
        "  def reset(self):\n",
        "    for layer in self.layers:\n",
        "      if layer.is_stateful():\n",
        "        layer.reset()\n",
        "\n",
        "  def __str__(self):\n",
        "    s = \"Sequential:\\n\"\n",
        "    for layer in self.layers:\n",
        "      s += str(layer) + \"\\n\"\n",
        "    return s"
      ],
      "metadata": {
        "id": "uLH_wjTSepPE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 96\n",
        "model = Sequential(vocab_size,\n",
        " [VanillaRNN(vocab_size, 500, batch_size, device='cuda'),\n",
        "  VanillaRNN(500, 400, batch_size, device='cuda'),\n",
        "  Linear(400, vocab_size, device='cuda')], device='cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uLhFjaLesWm",
        "outputId": "2addea04-c929-4342-9de6-fdd42ed20a7b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential:\n",
            "VanillaRNN: [torch.Size([576, 500]), torch.Size([1, 500])]\n",
            "VanillaRNN: [torch.Size([900, 400]), torch.Size([1, 400])]\n",
            "Linear: torch.Size([400, 76])\n",
            "\n",
            "Parameter Count: 679376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smooth_loss = -torch.log(torch.tensor(1.0/vocab_size, device='cuda')) # loss at iteration 0\n",
        "seq_length = 64\n",
        "optimizer = torch.optim.Adam(model.params)\n",
        "input_end = len(data) - seq_length\n",
        "for n in range(50000):\n",
        "  batch_starts = np.random.randint(0, input_end, batch_size)\n",
        "  # batch_size x seq_length\n",
        "  inputs = torch.tensor([[char_to_ix[ch] for ch in data[b_i : b_i + seq_length]] for b_i in batch_starts], device='cuda')\n",
        "  targets = torch.tensor([[char_to_ix[ch] for ch in data[b_i + 1 : b_i + seq_length + 1]] for b_i in batch_starts], device='cuda')\n",
        "  # sample from the model now and then\n",
        "  if n>0 and n%1000 == 0:\n",
        "    samples = model.sample(inputs[0, 0].detach(), 200)\n",
        "    # print(samples)\n",
        "    txt = ''.join(ix_to_char[ix.item()] for ix in samples)\n",
        "    print('*** Sample: ***')\n",
        "    print(f'----\\n{txt}\\n----')\n",
        "  # forward seq_length characters through the net and fetch gradient\n",
        "  logits = model(inputs) # batch_size x seq_length\n",
        "  loss = torch.tensor(0.0, device='cuda')\n",
        "  for b_i in range(batch_size):\n",
        "    loss += F.cross_entropy(logits[b_i], targets[b_i])\n",
        "  loss /= batch_size\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  smooth_loss = smooth_loss * 0.995 + loss * 0.005\n",
        "  if n>0 and n % 200 == 0:\n",
        "    print(f'iter: {n}, loss: {smooth_loss.item()}') # print progress\n",
        "    # plt.figure(figsize=(20, 4)) # width and height of the plot\n",
        "    # legends = []\n",
        "    for i, layer in enumerate(model.layers[:-1]): # note: exclude the output layer\n",
        "      if isinstance(layer, LinearRNN) or isinstance(layer, VanillaRNN):\n",
        "        t = layer.tanh.out\n",
        "        print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
        "    #     hy, hx = torch.histogram(t, density=True)\n",
        "    #     plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    #     legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
        "    # plt.legend(legends);\n",
        "    # plt.title('activation distribution')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODMbbiJ1e_oW",
        "outputId": "ffdde94b-fa36-4ed7-b165-f77ce8306262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 200, loss: 3.1576790809631348\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.18, saturated: 0.00%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.57, saturated: 3.85%\n",
            "iter: 400, loss: 2.3975062370300293\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.20, saturated: 0.00%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.68, saturated: 11.24%\n",
            "iter: 600, loss: 1.9926141500473022\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.22, saturated: 0.00%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.74, saturated: 19.28%\n",
            "iter: 800, loss: 1.7584495544433594\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.24, saturated: 0.00%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.78, saturated: 25.02%\n",
            "*** Sample: ***\n",
            "----\n",
            " picked his gran winder than inty like a tign, watches. \"At the blacked her endest'm through the rembarted himself fanes thicked the hurg. The hould flooks straightered at the crettencus kroce thind Ma\n",
            "----\n",
            "iter: 1000, loss: 1.6125104427337646\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.26, saturated: 0.00%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.81, saturated: 32.15%\n",
            "iter: 1200, loss: 1.5160889625549316\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.28, saturated: 0.00%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.82, saturated: 35.42%\n",
            "iter: 1400, loss: 1.4528254270553589\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.29, saturated: 0.00%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.84, saturated: 39.97%\n",
            "iter: 1600, loss: 1.4067978858947754\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.30, saturated: 0.00%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.85, saturated: 42.06%\n",
            "iter: 1800, loss: 1.3741021156311035\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.31, saturated: 0.00%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.85, saturated: 42.89%\n",
            "*** Sample: ***\n",
            "----\n",
            "Villager. He's a baugh, hamm's the river. As though the looked over Rand, but it surprised that realowared oyes swand the straemfy wided an ever had swyat vision. When it was fever'st'sp that had long \n",
            "----\n",
            "iter: 2000, loss: 1.3470286130905151\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.32, saturated: 0.00%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.86, saturated: 45.25%\n",
            "iter: 2200, loss: 1.3248778581619263\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.33, saturated: 0.00%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.86, saturated: 47.88%\n",
            "iter: 2400, loss: 1.30935800075531\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.34, saturated: 0.00%\n",
            "layer 1 (VanillaRNN): mean -0.03, std 0.87, saturated: 49.04%\n",
            "iter: 2600, loss: 1.2945042848587036\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.34, saturated: 0.01%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.87, saturated: 51.18%\n",
            "iter: 2800, loss: 1.2810251712799072\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.34, saturated: 0.01%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.88, saturated: 52.29%\n",
            "*** Sample: ***\n",
            "----\n",
            "t she looked my hat sommon the world, she touched the clopped to one else, after to me, we mass chased in for a good yet. Some.\"\n",
            "\n",
            "Nynaeve have find no mightine questions the Wes?\" Moiraine sound to do \n",
            "----\n",
            "iter: 3000, loss: 1.270917296409607\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.34, saturated: 0.01%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.88, saturated: 52.66%\n",
            "iter: 3200, loss: 1.2619160413742065\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.35, saturated: 0.02%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.89, saturated: 54.40%\n",
            "iter: 3400, loss: 1.2549378871917725\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.36, saturated: 0.02%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.88, saturated: 54.01%\n",
            "iter: 3600, loss: 1.2442317008972168\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.36, saturated: 0.04%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.89, saturated: 57.08%\n",
            "iter: 3800, loss: 1.2368180751800537\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.36, saturated: 0.04%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.89, saturated: 56.41%\n",
            "*** Sample: ***\n",
            "----\n",
            "e make up to it.\" He sounded like faded and down, as you walking that she waved with barred incletted an other point in more than his bowed. Only he washed to make it. Castant he and Lan said slowly. \"\n",
            "----\n",
            "iter: 4000, loss: 1.2305150032043457\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.37, saturated: 0.03%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.89, saturated: 57.13%\n",
            "iter: 4200, loss: 1.2264493703842163\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.38, saturated: 0.06%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.90, saturated: 57.95%\n",
            "iter: 4400, loss: 1.218260645866394\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.38, saturated: 0.06%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.90, saturated: 58.33%\n",
            "iter: 4600, loss: 1.2133071422576904\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.39, saturated: 0.09%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.90, saturated: 59.63%\n",
            "iter: 4800, loss: 1.2097575664520264\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.39, saturated: 0.08%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.90, saturated: 60.26%\n",
            "*** Sample: ***\n",
            "----\n",
            " the rug was cut down to her distance neither than knowing else. Suddenly flashed through the drove even getting on the pall in quarred cup al'Thor, drawing Trollocs. \"Others could almost thenkind his \n",
            "----\n",
            "iter: 5000, loss: 1.2065069675445557\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.39, saturated: 0.10%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.91, saturated: 61.31%\n",
            "iter: 5200, loss: 1.2010860443115234\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.40, saturated: 0.11%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.90, saturated: 60.88%\n",
            "iter: 5400, loss: 1.196921467781067\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.41, saturated: 0.10%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.91, saturated: 61.76%\n",
            "iter: 5600, loss: 1.1930360794067383\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.41, saturated: 0.15%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.91, saturated: 62.94%\n",
            "iter: 5800, loss: 1.1890699863433838\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.41, saturated: 0.15%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.90, saturated: 61.69%\n",
            "*** Sample: ***\n",
            "----\n",
            "ow you,\" Rand said in the water or not down, last free withounders did not say. I move two need my own court. A few things they started at the morning, and off the about Rand wondered what be learned a\n",
            "----\n",
            "iter: 6000, loss: 1.1845555305480957\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.42, saturated: 0.15%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.91, saturated: 63.16%\n",
            "iter: 6200, loss: 1.1792775392532349\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.43, saturated: 0.19%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.91, saturated: 62.45%\n",
            "iter: 6400, loss: 1.1758733987808228\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.42, saturated: 0.19%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.91, saturated: 63.10%\n",
            "iter: 6600, loss: 1.1731468439102173\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.42, saturated: 0.17%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.91, saturated: 64.38%\n",
            "iter: 6800, loss: 1.172609806060791\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.44, saturated: 0.28%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.91, saturated: 63.71%\n",
            "*** Sample: ***\n",
            "----\n",
            "im, year. Drives have had bet here. This is where she could get it op now. Or there in a colver. Rand hasucious hard, but the Aes Sedai sat said across the flame. \"Fare if you have to go,\" he said a ba\n",
            "----\n",
            "iter: 7000, loss: 1.1680618524551392\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.44, saturated: 0.26%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.91, saturated: 64.64%\n",
            "iter: 7200, loss: 1.1655372381210327\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.45, saturated: 0.31%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.92, saturated: 66.29%\n",
            "iter: 7400, loss: 1.1639446020126343\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.45, saturated: 0.29%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.91, saturated: 64.89%\n",
            "iter: 7600, loss: 1.1595866680145264\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.45, saturated: 0.29%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.92, saturated: 65.94%\n",
            "iter: 7800, loss: 1.1572816371917725\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.46, saturated: 0.40%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.92, saturated: 66.60%\n",
            "*** Sample: ***\n",
            "----\n",
            "s to reasture.\n",
            "\n",
            "Rand said.\"\n",
            "\n",
            "Rand laughed as he realized what he was dulling Rand. He belongan time they were tooce, working, for a few people and anything in Mandarb poollight excelech intently is a g\n",
            "----\n",
            "iter: 8000, loss: 1.1551928520202637\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.47, saturated: 0.46%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.92, saturated: 66.54%\n",
            "iter: 8200, loss: 1.151686191558838\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.46, saturated: 0.38%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.92, saturated: 66.78%\n",
            "iter: 8400, loss: 1.149390459060669\n",
            "layer 0 (VanillaRNN): mean +0.00, std 0.47, saturated: 0.48%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.92, saturated: 67.02%\n",
            "iter: 8600, loss: 1.1471788883209229\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.47, saturated: 0.51%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.92, saturated: 67.42%\n",
            "iter: 8800, loss: 1.1457384824752808\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.48, saturated: 0.55%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.92, saturated: 67.57%\n",
            "*** Sample: ***\n",
            "----\n",
            " for the Dark One. He felt the whole vasis no paybe a bit of rubbing his hands on Perrin?\"\n",
            "\n",
            "\"There is on her.\"\n",
            "\n",
            "\"Have you voo man? \"A leashed a man just as most of the Father than Thom want to trap. He\n",
            "----\n",
            "iter: 9000, loss: 1.1448237895965576\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.49, saturated: 0.58%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.92, saturated: 68.90%\n",
            "iter: 9200, loss: 1.1401801109313965\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.49, saturated: 0.64%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.92, saturated: 67.67%\n",
            "iter: 9400, loss: 1.1380646228790283\n",
            "layer 0 (VanillaRNN): mean +0.00, std 0.49, saturated: 0.69%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.92, saturated: 68.62%\n",
            "iter: 9600, loss: 1.1376382112503052\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.49, saturated: 0.59%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.92, saturated: 67.59%\n",
            "iter: 9800, loss: 1.1344594955444336\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.49, saturated: 0.62%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.92, saturated: 68.23%\n",
            "*** Sample: ***\n",
            "----\n",
            "pulled the wind doubled heriff toward the forehta with a cold clothes beyond the Trolloc out Mandan bared the Willow pale small in nobody knew about, but he was claiming flesh, it was minching. He's my\n",
            "----\n",
            "iter: 10000, loss: 1.1332019567489624\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.50, saturated: 0.81%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.92, saturated: 68.32%\n",
            "iter: 10200, loss: 1.1299649477005005\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.50, saturated: 0.70%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.92, saturated: 68.94%\n",
            "iter: 10400, loss: 1.1265629529953003\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.51, saturated: 0.92%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.92, saturated: 68.82%\n",
            "iter: 10600, loss: 1.1271085739135742\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.51, saturated: 0.90%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.92, saturated: 69.14%\n",
            "iter: 10800, loss: 1.124777913093567\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.51, saturated: 0.99%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.93, saturated: 69.54%\n",
            "*** Sample: ***\n",
            "----\n",
            "t. Light, it should go in Sarain Cattianne, not that day that had gone with thicket half-legged toge-faced and never was broken on the whole after her after leafly glance. \"It is my founcher and the My\n",
            "----\n",
            "iter: 11000, loss: 1.12302827835083\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.52, saturated: 1.03%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.93, saturated: 68.91%\n",
            "iter: 11200, loss: 1.121030330657959\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.52, saturated: 1.08%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.93, saturated: 70.01%\n",
            "iter: 11400, loss: 1.1192950010299683\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.53, saturated: 1.09%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.93, saturated: 69.96%\n",
            "iter: 11600, loss: 1.1184967756271362\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.52, saturated: 1.21%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.93, saturated: 70.56%\n",
            "iter: 11800, loss: 1.1170852184295654\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.53, saturated: 1.19%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.93, saturated: 70.88%\n",
            "*** Sample: ***\n",
            "----\n",
            "s face, but there was nothing wildly got breasing with the earth found in the wind swat from the human with the small way to a rock, she slowed as tending one one power, and he put the blanket to get a\n",
            "----\n",
            "iter: 12000, loss: 1.1162689924240112\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.53, saturated: 1.27%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.93, saturated: 71.88%\n",
            "iter: 12200, loss: 1.1132440567016602\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.53, saturated: 1.36%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.93, saturated: 70.12%\n",
            "iter: 12400, loss: 1.1132196187973022\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.53, saturated: 1.30%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.93, saturated: 71.16%\n",
            "iter: 12600, loss: 1.1098755598068237\n",
            "layer 0 (VanillaRNN): mean -0.01, std 0.54, saturated: 1.29%\n",
            "layer 1 (VanillaRNN): mean -0.01, std 0.93, saturated: 71.29%\n",
            "iter: 12800, loss: 1.108983039855957\n",
            "layer 0 (VanillaRNN): mean -0.00, std 0.54, saturated: 1.32%\n",
            "layer 1 (VanillaRNN): mean -0.02, std 0.93, saturated: 72.19%\n"
          ]
        }
      ]
    }
  ]
}